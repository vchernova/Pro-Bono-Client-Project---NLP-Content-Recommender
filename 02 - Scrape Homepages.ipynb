{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping notes\n",
    "I used BeautifulSoup library for webscraping and EC2 on AWS to run it. This part actually took quite long, because I needed to account for many errors (including both request errors and issues with the entered url), so often the code would run for several hours and then break. I ended up batching it first. And then reran as a whole df.\n",
    "\n",
    "I recorded both info from the homepage and the status errors, to consider whether the urls provided were no longer in existence. This also contributed to the \"freshness\" of the client company info, which I will discuss in the futher notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./clean_urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>co_id</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0012ad11-e1fb-4fa1-85fd-33bd428f0427</td>\n",
       "      <td>http://www.anovavoz.com.br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00150029-74d1-4040-978b-f84116ffe568</td>\n",
       "      <td>http://markiapp.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016ce4f-eee4-43e8-8913-32ce35689f0e</td>\n",
       "      <td>Https://zoisolar.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0016db7b-45a5-48bb-a5d2-7157a46d283f</td>\n",
       "      <td>http://www.abi-africa.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0017ede6-3113-403a-ae6f-4ec1517f2e04</td>\n",
       "      <td>http://www.jurispprudence.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  co_id                        website\n",
       "2  0012ad11-e1fb-4fa1-85fd-33bd428f0427     http://www.anovavoz.com.br\n",
       "3  00150029-74d1-4040-978b-f84116ffe568            http://markiapp.com\n",
       "4  0016ce4f-eee4-43e8-8913-32ce35689f0e           Https://zoisolar.com\n",
       "5  0016db7b-45a5-48bb-a5d2-7157a46d283f      http://www.abi-africa.com\n",
       "6  0017ede6-3113-403a-ae6f-4ec1517f2e04  http://www.jurispprudence.com"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scraping function: ping the website to check if it's still alive, scrape homepage, record error message if there is one.\n",
    "\n",
    "The reason for recording error messages to later see whether a company's url was 403 (website no longer available) and note that those clients are potentially no longer there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_soup(url):\n",
    "    import idna\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        status_code = res.status_code\n",
    "        url = res.url\n",
    "        error = \"No error\"\n",
    "        soup = (BeautifulSoup(res.content, \"html5lib\"))\n",
    "        print(url, error, status_code)\n",
    "    except requests.ConnectionError as e:\n",
    "        error = (\"CONNECTION ERROR: \" + str(e))\n",
    "        url = url\n",
    "        status_code = \"Error1\"\n",
    "        soup = \"Error1\"\n",
    "        print(url, error, status_code)\n",
    "    except idna.IDNAError as e:\n",
    "        error = (\"IDNA ERROR:\" + str(e))\n",
    "        url = url\n",
    "        status_code = \"Error2\"\n",
    "        soup = \"Error2\"\n",
    "        print(url, error, status_code)\n",
    "    except requests.exceptions.ReadTimeout as e:\n",
    "        error = (\"TIMEOUT ERROR:\" + str(e))\n",
    "        url = url\n",
    "        status_code = \"Error3\"\n",
    "        soup = \"Error3\"\n",
    "        print(url, error, status_code)\n",
    "   \n",
    "    return (url, status_code, error, soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping (note I did in batches using EC2 on AWS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN, UNLESS YOU ARE RERUNNING ON PURPOSE. IT'LL TAKE A REALLY LONG TIME.\n",
    "results = df['website'].apply(scrape_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split that data into status code (to see whether the website is still running) and the rest (for future processing)\n",
    "df['url'] = [result[0] for result in results]\n",
    "df['status_code'] = [result[1] for result in results]\n",
    "df['error'] = [result[2] for result in results]\n",
    "df['soup'] = [result[3] for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. URLs with info from scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_pickle(\"./FINAL_pickle_soup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200       6884\n",
       "Error1     611\n",
       "403        392\n",
       "404        154\n",
       "429         32\n",
       "503         23\n",
       "500         15\n",
       "504          8\n",
       "401          7\n",
       "412          4\n",
       "999          3\n",
       "502          3\n",
       "410          3\n",
       "416          3\n",
       "526          3\n",
       "Error2       2\n",
       "Error3       2\n",
       "402          2\n",
       "406          2\n",
       "524          1\n",
       "522          1\n",
       "409          1\n",
       "424          1\n",
       "203          1\n",
       "477          1\n",
       "418          1\n",
       "521          1\n",
       "Name: status_code, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.status_code.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 8K+ urls just under 7K came back with status code 200, around 400 with status code 403 and the rest with difference status code errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
