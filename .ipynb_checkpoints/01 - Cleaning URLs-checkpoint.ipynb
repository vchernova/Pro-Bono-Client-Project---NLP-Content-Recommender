{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about this JN\n",
    "One of the suggestions about filling in the missing data was to use the URLS for the cliemts that provided them and then scrape the homepages (considered going 2 layers deep but decided to focus just on the homepage) of the websites and get social media handles that we needed, plus possible company descriptions.\n",
    "\n",
    "So in preparation for that step I cleaned the urls. Technically every client had a url entered, but after cleaning we saw that many used incorrect addresses (willingly and by error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "df = pd.read_csv(\"../capstone/XXXXXX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape #originally under 20K observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.website = df.website.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dropping duplicates, rows where url was not provided, and if the email was provided instead of the url - drop the name prepend and apply url schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with no url\n",
    "df.dropna(subset=['website'], inplace = True)\n",
    "\n",
    "#dropping duplicates\n",
    "df = df.drop_duplicates(subset='website', keep='first')\n",
    "\n",
    "#those who used email instead of url, just keep the url the part after @ and prepend with the correct schema\n",
    "import re\n",
    "for i in df[df['website'].str.contains(\"@\", na = False)].index:\n",
    "    df.website[i] = \"http://\" + (df.website[i].split('@')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #this cut # of observations by 40%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dropping observations where social media accounts, app store links, were used instead of url\n",
    "\n",
    "##### This is not fool-proff, I am sure I am unaware of some other social media accounds that people randomly used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows where twitter account used instead of url\n",
    "df = df[df.website.str.contains(\"twitt\") == False]\n",
    "\n",
    "#dropping rows where linkedin account used instead of url\n",
    "df = df[df.website.str.contains(\"linkedin\") == False]\n",
    "\n",
    "#dropping rows where google account used instead of url (including google.com, docs, apps etc.)\n",
    "df = df[df.website.str.contains(\"play.google\") == False]\n",
    "df = df[df.website.str.contains(\"www.google\") == False]\n",
    "df = df[df.website.str.contains(\"drive.google\") == False]\n",
    "df = df[df.website.str.contains(\"docs.google\") == False]\n",
    "df = df[df.website.str.contains(\"//google\") == False]\n",
    "df = df[df.website.str.contains(\"www.google\") == False]\n",
    "df = df[df.website.str.contains(\"gmail\") == False]\n",
    "df = df[df.website.str.contains(\"plus.google\") == False]\n",
    "\n",
    "#dropping yahoo searches and email accounts\n",
    "df = df[df.website.str.contains(\"yahoo\") == False]\n",
    "\n",
    "#dropping hotmail searches and email accounts\n",
    "df = df[df.website.str.contains(\"hotm\") == False]\n",
    "\n",
    "#dropping rows where facebook account used instead of url\n",
    "df = df[df.website.str.contains(\"facebook\") == False]\n",
    "\n",
    "#dropping rows where instagram account used instead of url\n",
    "df = df[df.website.str.contains(\"instagram\") == False]\n",
    "\n",
    "#dropping rows where YouTube account used instead of url\n",
    "df = df[df.website.str.contains(\"youtu\") == False]\n",
    "\n",
    "#dropping rows where Whatsapp account used instad of url\n",
    "df = df[df.website.str.contains(\"whatsapp\") == False]\n",
    "\n",
    "#dropping rows where Reddit account used instad of url\n",
    "df = df[df.website.str.contains(\"reddit\") == False]\n",
    "\n",
    "#dropping rows where applestore redirect used instad of url\n",
    "df = df[df.website.str.contains(\"//apple.co\") == False]\n",
    "\n",
    "#dropping rows where they used links to your company pages with their descriptions (pages now redirect to your homepage)\n",
    "df = df[df.website.str.contains(\"XXX.vc\")== False]\n",
    "\n",
    "#dropping rows where verizon account used instad of url\n",
    "df = df[df.website.str.contains(\"verizon\") == False]\n",
    "\n",
    "#dropping rows where aol account used instad of url\n",
    "df = df[df.website.str.contains(\"aol.com\") == False]\n",
    "\n",
    "#dropping rows where ATT account used instad of url\n",
    "df = df[df.website.str.contains(\"ATT.NET\") == False]\n",
    "\n",
    "#dropping rows where people wrote in some variation of none.com\n",
    "df = df[df.website.str.contains(\"none\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for possible incosistent emails here\n",
    "df = df[df.website.str.contains(\"google.vc\")== True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. This is not ideal, but fixing some by hand to preserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing missplled urls\n",
    "df.loc[df.website == \"XXX.com\", 'website'] = \"http://XXX.com/\"\n",
    "df.loc[df.website == \"XXX.com\", 'website'] = \"https://www.XXXX.com/\"\n",
    "df.loc[df.website == \"XXX.com\", 'website'] = \"http://www.XXX.com\"\n",
    "df.loc[df.website == \"XXX\", 'website'] = \"https://XXX.systems/\"\n",
    "df.loc[df.website == \"XXX\", 'website'] = \"https://www.XXX.com/\"\n",
    "df.loc[df.website == \"XXX\", 'website'] = \"http://www.XXX.tv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #after cleaning have just over 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just using company id and website columns for webscraping, so saving that slice\n",
    "df2 = df[['co_id', 'website']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_pickle('clean_urls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
