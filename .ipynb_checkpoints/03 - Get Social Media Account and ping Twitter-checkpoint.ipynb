{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook description\n",
    "\n",
    "In this notebook I first parsed out the social media account fetched from scraping the homepages of the clients' urls. Then used the collected twitter handles to check the twitter accounts and get last tweet, to add more words to text for the nlp piece I will do later. AND to get more info about the \"freshness\" of the company info.\n",
    "\n",
    "One of the things the startup I working with was interested about was to know whether the info provided to them by the clients was still accurate. By checking urls and twitter, we could check whether the url still works and when the last twit was posted. That could give an idea of whether the info about the client is still accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the pickled df with the urls to look for twitter handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./FINAL_pickle_soup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get twitter and other social media links from the homepage scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_links = []\n",
    "for i in df.index:\n",
    "    facebook_links.append([elem['href'] for elem in BeautifulSoup(df['soup'][i],\n",
    "                'lxml').find_all('a') if 'href' in elem.attrs and \"facebook\" in elem['href']])\n",
    "df['facebook'] = facebook_links    \n",
    "\n",
    "print(\"No facebook found {}\".format (len(df.loc[np.array(list(map(len,df.facebook.values)))==0])))\n",
    "print (\"1 facebook found {}\".format (len(df.loc[np.array(list(map(len,df.facebook.values)))==1])))\n",
    "print (\"More than 1 facebook found {}\".format (len(df.loc[np.array(list(map(len,df.facebook.values)))>1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_links = []\n",
    "for i in df.index:\n",
    "    twitter_links.append([elem['href'] for elem in BeautifulSoup(df['soup'][i],\n",
    "                'lxml').find_all('a') if 'href' in elem.attrs and \"twitter\" in elem['href']])\n",
    "df['twitter'] = twitter_links\n",
    "\n",
    "print(\"No twitter links found {}\".format (len(df.loc[np.array(list(map(len,df.twitter.values)))==0])))\n",
    "print (\"One twitter link found {}\".format (len(df.loc[np.array(list(map(len,df.twitter.values)))==1])))\n",
    "print (\"More than 1 twitter links found {}\".format (len(df.loc[np.array(list(map(len,df.twitter.values)))>1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_links = []\n",
    "for i in df.index:\n",
    "    youtube_links.append([elem['href'] for elem in BeautifulSoup(df['soup'][i],\n",
    "                'lxml').find_all('a') if 'href' in elem.attrs and \"youtu\" in elem['href']])\n",
    "df['youtube'] = youtube_links\n",
    "\n",
    "print(\"No youtube links found {}\".format (len(df.loc[np.array(list(map(len,df.youtube.values)))==0])))\n",
    "print (\"One youtube link found {}\".format (len(df.loc[np.array(list(map(len,df.youtube.values)))==1])))\n",
    "print (\"More than 1 youtube link found {}\".format (len(df.loc[np.array(list(map(len,df.youtube.values)))>1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instagram_links = []\n",
    "for i in df.index:\n",
    "    instagram_links.append([elem['href'] for elem in BeautifulSoup(df['soup'][i],\n",
    "                'lxml').find_all('a') if 'href' in elem.attrs and \"instagram\" in elem['href']])\n",
    "df['instagram'] = instagram_links\n",
    "\n",
    "print(\"No instagram links found {}\".format (len(df.loc[np.array(list(map(len,df.instagram.values)))==0])))\n",
    "print (\"One instagram link found {}\".format (len(df.loc[np.array(list(map(len,df.instagram.values)))==1])))\n",
    "print (\"More than 1 instagram link found {}\".format (len(df.loc[np.array(list(map(len,df.instagram.values)))>1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "text = []\n",
    "for i in df.index:\n",
    "    text.append(BeautifulSoup(df['soup'][i],'lxml').text.replace(\"\\n\", \"\"))\n",
    "\n",
    "df['homepage_text'] = text\n",
    "\n",
    "print(\"No text found {}\".format (len(df.loc[np.array(list(map(len,df.instagram.values)))==0])))\n",
    "print (\"Some text found {}\".format (len(df.loc[np.array(list(map(len,df.instagram.values)))>=1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the timestamp and text of the last tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I used selenium\n",
    "driver = webdriver.Chrome(executable_path=\"../chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the pinned one comes up first, then the first one on the page. Total of 19. Iam only keeping the 1st one.\n",
    "#opens a new browser window for each handle in the list \n",
    "#takes about 5 sec per handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REWRITE THIS FOR SCALE\n",
    "# THROW ON AWS\n",
    "\n",
    "dates = []\n",
    "tweet = []\n",
    "driver = webdriver.Chrome(executable_path=\"../chromedriver\")\n",
    "for i in (twitters):\n",
    "    driver.get(i)\n",
    "    dates.append([element.get_attribute('title') for element in driver.find_elements_by_xpath('//a[starts-with(@class, \"tweet-timestamp js-permalink js-nav js-tooltip\")]')])\n",
    "    tweet.append(driver.find_element_by_class_name('tweet').text)   \n",
    "    sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
