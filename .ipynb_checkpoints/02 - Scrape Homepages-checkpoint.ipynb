{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping notes\n",
    "I used BeautifulSoup library for webscraping and EC2 on AWS to run it. This part actually took quite long, because I needed to account for many errors (including both request errors and issues with the entered url), so often the code would run for several hours and then break. I ended up batching it first. And then reran as a whole df.\n",
    "\n",
    "I recorded both info from the homepage and the status errors, to consider whether the urls provided were no longer in existence. This also contributed to the \"freshness\" of the client company info, which I will discuss in the futher notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./clean_urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scraping function: ping the website to check if it's still alive, scrape homepage, record error message if there is one.\n",
    "\n",
    "The reason for recording error messages to later see whether a company's url was 403 (website no longer available) and note that those clients are potentially no longer there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_soup(url):\n",
    "    import idna\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        status_code = res.status_code\n",
    "        url = res.url\n",
    "        error = \"No error\"\n",
    "        soup = (BeautifulSoup(res.content, \"html5lib\"))\n",
    "        print(url, error, status_code)\n",
    "    except requests.ConnectionError as e:\n",
    "        error = (\"CONNECTION ERROR: \" + str(e))\n",
    "        url = url\n",
    "        status_code = \"Error1\"\n",
    "        soup = \"Error1\"\n",
    "        print(url, error, status_code)\n",
    "    except idna.IDNAError as e:\n",
    "        error = (\"IDNA ERROR:\" + str(e))\n",
    "        url = url\n",
    "        status_code = \"Error2\"\n",
    "        soup = \"Error2\"\n",
    "        print(url, error, status_code)\n",
    "    except requests.exceptions.ReadTimeout as e:\n",
    "        error = (\"TIMEOUT ERROR:\" + str(e))\n",
    "        url = url\n",
    "        status_code = \"Error3\"\n",
    "        soup = \"Error3\"\n",
    "        print(url, error, status_code)\n",
    "   \n",
    "    return (url, status_code, error, soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping (note I did in batches using EC2 on AWS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN, UNLESS YOU ARE RERUNNING ON PURPOSE. IT'LL TAKE A REALLY LONG TIME.\n",
    "results = df['website'].apply(scrape_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split that data into status code (to see whether the website is still running) and the rest (for future processing)\n",
    "df['url'] = [result[0] for result in results]\n",
    "df['status_code'] = [result[1] for result in results]\n",
    "df['error'] = [result[2] for result in results]\n",
    "df['soup'] = [result[3] for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. URLs with info from scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_pickle(\"./FINAL_pickle_soup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.status_code.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out all the urls ust under 90% came back with status code 200, around 5% with status code 403 and the rest with difference status code errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
