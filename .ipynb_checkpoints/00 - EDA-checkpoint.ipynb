{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about the project\n",
    "At the beginning of the project we did some EDA on the data available. We agreed that the Data was sparse and required extensive cleaning and filling in the gaps, which took most of the time working on the project.\n",
    "\n",
    "Here is an abbreviated noteboo showing some parts of the EDA. Unfortunately the data itself is not available but you can get an idea of the process from the steps below.\n",
    "\n",
    "First I visuzlied the key findings about the dataset (what's available, what's missing etc.), then after I talked to the team I did some graphs on the items that they were most interested about and suggested a few ways of how we could get additional data and compensate for missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE That the data isn't available as it is propriatary for the startup I worked with\n",
    "#loading data\n",
    "df = pd.read_csv(\"./XXX DATA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the data\n",
    "\n",
    "Data was merged from several tables on co_id field and very sparse. Different sets available for different observations. For example certain features were only available for less than 5% of the dataset. While other features were available for virtually all observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check how many missing values in each column in the df, put in df_view so it's easier to see\n",
    "#can't do df.decribe bc it only works on integers and floats and most of these are strings\n",
    "\n",
    "df_view = pd.DataFrame(columns=['columns'])\n",
    "df_view['columns'] = df.columns\n",
    "df_view['missing_records'] = [df[i].isnull().sum() for i in df.columns]\n",
    "df_view['all_records'] = [len(df[i]) for i in df.columns]\n",
    "df_view['%missing_records'] = df_view[\"missing_records\"] / df_view['all_records']\n",
    "df_view['number_unique_records'] = [df[i].nunique() for i in df.columns]\n",
    "\n",
    "df_view.sort_values('missing_records', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking how long the clients have been using the platform\n",
    "First calculating the length from signing up until the current date (validated with the startup that that's the approach they would like to use here). Then plotting it for vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "df['created'] = pd.to_datetime(df['created'])\n",
    "df['updated'] = pd.to_datetime(df['updated'])\n",
    "df['time_using_platform'] = date.today() - df['updated']\n",
    "df['updated'] = pd.to_datetime(df['time_using_platform'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.time_using_platform.value_counts()[:5]\n",
    "#what happened 253 (Oct 31, 2017) and 803 (Apr 29, 2016) days ago? Spikes in account creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "bins = np.arange(0,850,10)\n",
    "plt.hist([np.clip(df.time_using_platform.astype('timedelta64[D]'), bins[0], bins[-1])],\n",
    "                                density=True,\n",
    "                                bins=bins, facecolor='b');\n",
    "\n",
    "plt.title('#Days that the Companies spend on the platform')\n",
    "plt.xlabel('# Days')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.xticks (np.arange(0, 850, step=25))\n",
    "#plt.yticks (np.arange(0, 2200, step=100))\n",
    "#plt.axis([0, 900, 0, 0.05])\n",
    "plt.grid(True, alpha = 0.25 )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting for the NLP features\n",
    "Since I will be using text for my recommender engine later, I am not looking at the available text fields to then decide witht the client how much text do we need at a minimum for them to make a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions - since there are thee columns with descriptions, I am combining it into one, \n",
    "# so I can decide whether a company provided a description at all\n",
    "df['text_description'] = df[\"description\"].astype(str) + \"\" + df['short_description'].astype(str) + \"\" + df['concept_pitch'].astype(str)\n",
    "\n",
    "\n",
    "# adding length of description column for easier plottings\n",
    "length = []\n",
    "for i in df.index:\n",
    "    length.append(len(df.text_description[i].split()) - 1) #-1 to get rid of NaNs\n",
    "    \n",
    "df['text_length'] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['text_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#looking at companies that did not provide any description\n",
    "print(\"{} % of companies did not provide company description.\".format(sum(df.text_length == 0)/ len(df.text_length))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here are the top ten description lengths\n",
    "df.text_length.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out someone submitted a copy of their homepage html instead of description, but other than that all entries were less than 216 words long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "bins = np.arange(0,100,5)\n",
    "plt.hist([np.clip(df.text_length, bins[0], bins[-1])],\n",
    "                                density=True,\n",
    "                                bins=bins, color=['#3782CC'])\n",
    "plt.title('Distribution of Company Description Length of Text Distribution')\n",
    "plt.xlabel('Text Length / # Words')\n",
    "plt.ylabel('% Companies that have that length of description')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at correlations between some features\n",
    "Cehcking whether the length of company description correlates with the length of the use of the platform. More interesting correlations were found with other features but not including here for proprietary info reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tu = df[['time_using_platform', 'text_length']]\n",
    "df_tu['time_using_platform'] = df_tu['time_using_platform'].astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df_tu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tu.length_using_platform = round(df_round.length_using_platform)\n",
    "pd.crosstab(df_tu.text_length, df_round.length_using_platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so this could be cool to know. right now most \"XXX\" info is missing and only a few hundred crosstab hits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
