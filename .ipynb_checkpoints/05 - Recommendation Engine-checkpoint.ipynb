{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook description\n",
    "\n",
    "This is the most interesting part of the project. I am still working on it and may update this notebook at a later date.\n",
    "\n",
    "We used a content recommender to approximate a search engine. Basically saying, recommend me the most similar text blob to the one I enter as a seach term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crunchbase\n",
    "\n",
    "I used crunchbase data to test the approach, since crunchbase data is already cleaned and organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crunch = pd.read_csv('./organizations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crunch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf.fit(union['get_text'].values.astype(str))\n",
    "tf_matrix = tfidf.transform(union['get_text'].values.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['SAMPLE RESEARCH TERMS']\n",
    "search_vector = tfidf.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(tf_matrix, search_vector, metric = 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crunch.loc[distances.argmin(),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df = pd.DataFrame(tf_matrix.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Startup data: merge with scraped text and tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pd.read_csv('./git_projects/capstone/XXX.csv')\n",
    "scrape = pd.read_pickle('./git_projects/capstone/FINAL_pickle_soup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the soup\n",
    "\n",
    "Clean up the tags and other non-text in the scraped homepages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from bs4.element import Comment\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'lxml')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_soup = []\n",
    "for i in scrape.index:\n",
    "    soup = BeautifulSoup(scrape['soup'][i], \"lxml\")\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    clean_soup.append(u\" \".join(t.strip() for t in visible_texts))\n",
    "    \n",
    "scrape['homepage'] = clean_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = scrape.drop(['soup'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pd.merge(client, scrape, on='co_id', how='outer')\n",
    "client.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all nans, so they don't clutter the text\n",
    "client.replace(np.nan, '', regex=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that I will not use for TFIDF\n",
    "client = client.drop(['co_slug', 'created','updated',\n",
    "                   'website_x', 'website_y','url', 'email', ...'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correcting some fields (for ex people entered both city and state in the city field etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client['state'] = [i.split(',')[1:] for i in client['city']]\n",
    "client['state'] = client['state'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client['city'] = [i.split(',')[:1] for i in client['city']]\n",
    "client['city']=client['city'].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_hiphens(column):\n",
    "    client[column] = [' '.join(i.split('-')) for i in client[column]]    \n",
    "\n",
    "columns = ['XXX', 'YYY' etc....]\n",
    "for i in columns:\n",
    "    drop_hiphens(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving at this stage\n",
    "client.to_pickle(\"client_ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pd.read_pickle(\"./client_ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all the text in one column. Creating several combinations to see what works best with the recommendation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client['sum_description'] = (client['first feature'].astype(str) + \" \" + client['second feature'].astype(str)\n",
    "                     + \" \" + client['third feature'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['text_no_soup'] = (client['first feature'].astype(str) + \" \" + client['second feature'].astype(str)\n",
    "                     + \" \" + client['fourth feature'].astype(str) + \" \" client['fifth feature'].astype(str)\n",
    "                     + \" \" + client['second feature'].astype(str)\n",
    "                     + \" \" + client['seventh feature'].astype(str) + \" \"  + client['first feature'].astype(str)\n",
    "                     + \" \" + client['twirteenth feature'].astype(str)\n",
    "                     + \" \" + client['tenth feature'].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Client data TFIDF\n",
    "\n",
    "Here we are using a recommender to approximate searcg engine. TF-IDF to give weight to the words that are less frequent and specific to a given description and penalize the words that appear everywhere.\n",
    "\n",
    "We fit the vectorizer on the text from all description and then use the search term to find description that are most similar to the search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Also used lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a content-based recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting tfidf on our camlany descriptions\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "tfidf.fit(client['sum_description'].values.astype(str))\n",
    "tf_matrix = tfidf.transform(client['sum_description'].values.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming our search term\n",
    "y = [\"Sample search exaple\"]\n",
    "search_vector = tfidf.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding which descriptions are most similar to the searcg term\n",
    "distances = pairwise_distances(tf_matrix, search_vector, metric = 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(distances, axis=0)[:5] #five best scores/pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {} \n",
    "\n",
    "for idx, row in client.iterrows(): #\n",
    "    \n",
    "    similar_indices = np.sort(distances[idx].argsort()) #stores 5 most similar blocks of text\n",
    "    similar_items = [(distances[idx][i], client['co_id'][i]) for i in similar_indices]\n",
    "    results[row['co_id']] = similar_items[0][0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series(results, name='cosine_score')\n",
    "s.index.name = 'co_id'\n",
    "s = s.reset_index()\n",
    "s.sort_values('cosine_score', ascending = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a df that shows the clients that best match the search terms, organized from the best match to the worst.\n",
    "#We can see both the % similarity to the searh terms and the actual description.\n",
    "\n",
    "merged_df = pd.merge(union, s, on='co_id', how='outer')\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "merged_df[[\"co_name\",\"sum_description\", 'cosine_score']].sort_values('cosine_score', ascending = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
